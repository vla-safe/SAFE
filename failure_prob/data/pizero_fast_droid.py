'''
Dataloader for rollout datasets generated by pi0 with FAST Tokenizer. 
Note that for pi0-fast, action prediction is mostly treated as a sentence prediction tasks as well. 

pi0 is from the following repo:
https://github.com/Physical-Intelligence/openpi
'''

from collections import defaultdict
import glob
import os
import pickle
from pathlib import Path
import re
from typing import Optional

import cv2
import imageio
import natsort
import numpy as np
import pandas as pd
import torch
from tqdm import tqdm

# import matplotlib
# matplotlib.use('TkAgg') # Use an interactive backend for plotting
# from matplotlib import pyplot as plt

from failure_prob.conf import Config
from failure_prob.utils.failure_metrics import compute_token_metrics
from .utils import Rollout, set_task_min_step, split_rollouts_by_seen_unseen, process_tensor_idx_rel
from .openvla import split_rollouts as split_rollouts_openvla
from .pizero_fast import compute_hand_crafted_metrics

TASK_DESC_TO_STANDARD_LENGTH = {
    "close the drawer": 25,
}

EXCLUDE_TASK_DESCS = [
    # "unfold the cloth",
]


def parse_policy_record_paths(policy_record_paths: list[str]) -> pd.DataFrame:
    # First parse filename of all policy records into a df
    df_policy_records = []
    for policy_record_path in policy_record_paths:
        fname = os.path.basename(policy_record_path)
        pattern = re.compile(
            r'^step_(?P<global_step>\d+)--'       # captures the global step
            r'(?P<run_name>.*?)--'               # captures the run name (non‑greedy)
            r'task_(?P<task_id>\d+)--'           # captures the task id
            r'ep_(?P<ep_id>\d+)--'               # captures the episode id
            r't_(?P<step>\d+)--meta\.pkl$'       # captures the inner step
        )
        parsed = pattern.match(fname)
        if parsed:
            df_policy_records.append({
                "global_step": int(parsed.group("global_step")),
                "run_name": parsed.group("run_name"),
                "task_id": int(parsed.group("task_id")),
                "ep_id": int(parsed.group("ep_id")),
                "step": int(parsed.group("step")),
                "path": policy_record_path,
            })
        else:
            print(f"Warning: Unable to parse filename {fname}")
    df_policy_records = pd.DataFrame(df_policy_records)
    df_policy_records = df_policy_records.sort_values(by=["global_step"])
    return df_policy_records


def parse_env_record_path(fname: str) -> dict:
    pattern = re.compile(
        r'^task(?P<task_id>\d+)--'    # “task” plus digits
        r'ep(?P<ep_id>\d+)--'         # “ep” plus digits
        r'succ(?P<succ>\d+)--meta\.pkl$'  # “succ” plus digits, then “--meta.pkl”
    )

    m = pattern.match(fname)
    if not m:
        raise ValueError(f"Filename {fname!r} didn’t match the expected pattern")

    parts = m.groupdict()
    task_id = int(parts["task_id"])
    ep_id = int(parts["ep_id"])
    succ = int(parts["succ"])
    return task_id, ep_id, succ
    



def load_rollouts_from_root(load_root: Path, cfg: Config) -> list[Rollout]:
    '''
    Saved dict
    observation/state <class 'numpy.ndarray'> (8,) float64
    prompt <class 'str'> put both the alphabet soup and the tomato sauce in the basket
    actions <class 'numpy.ndarray'> (10, 7) float64
    decode_step <class 'numpy.ndarray'> () int32
    encoded <class 'numpy.ndarray'> (17, 2048) bfloat16
    logits <class 'numpy.ndarray'> (17, 2048) bfloat16
    pre_logits <class 'numpy.ndarray'> (17, 2048) bfloat16
    raw_actions <class 'numpy.ndarray'> (17,) float32
    state <class 'numpy.ndarray'> (8,) float32
    action_start_index_in_vocab <class 'int'> 254976
    action_end_index_in_vocab <class 'int'> 257024
    '''
    env_records_folder = load_root / "env_records"
    policy_records_folder = load_root / "policy_records"
    assert env_records_folder.exists(), f"Path {env_records_folder} does not exist"
    assert policy_records_folder.exists(), f"Path {policy_records_folder} does not exist"
    env_record_paths = glob.glob(str(env_records_folder / "*.pkl"))
    policy_record_paths = glob.glob(str(policy_records_folder / "*--meta.pkl"))
    assert len(env_record_paths) > 0, f"No env records found in {env_records_folder}"
    assert len(policy_record_paths) > 0, f"No policy records found in {policy_records_folder}"
    env_record_paths = natsort.natsorted(env_record_paths)
    policy_record_paths = natsort.natsorted(policy_record_paths)
    
    df_policy_records = parse_policy_record_paths(policy_record_paths)
    
    all_rollouts = []

    # for env_record_path in tqdm(env_record_paths, desc="Loading rollouts"):
    for env_record_path in env_record_paths:
        # Load the meta data from the env record
        env_record = pickle.load(open(env_record_path, "rb"))
        mp4_path = env_record_path.replace("meta.pkl", "external_left.mp4")
        
        episode_idx = env_record["episode_idx"]
        
        df_policy_records_this_eps = df_policy_records[
            df_policy_records["ep_id"] == episode_idx
        ]
        
        policy_records = []
        for _, row in df_policy_records_this_eps.iterrows():
            policy_record_path = row["path"]
            policy_record = pickle.load(open(policy_record_path, "rb"))
            policy_records.append(policy_record)
        
        # Extract hidden states and actions from policy records
        hidden_states = []
        action_vectors = []
        for policy_record in policy_records:
            hidden_state = policy_record[cfg.dataset.feat_name] # (n_tokens, dim_feat)
            
            # handle the token dimension
            # (n_tokens, dim_feat) -> (dim_feat)
            hidden_state = process_tensor_idx_rel(hidden_state, cfg.dataset.token_idx_rel)

            hidden_states.append(hidden_state)
            
            pred_horizon = policy_record["actions"].shape[0]
            dim_action = policy_record["actions"].shape[1]
            action = policy_record["actions"].reshape(-1) # (pred_horizon*action_dim)
            action_vectors.append(action)
        hidden_states = np.stack(hidden_states, axis=0).astype(np.float32)
        hidden_states = torch.from_numpy(hidden_states) # (n_steps, hidden_dim)
        action_vectors = np.stack(action_vectors, axis=0).astype(np.float32)
        action_vectors = torch.from_numpy(action_vectors) # (n_steps, pred_horizon*action_dim)
        
        cfg.dataset.dim_features = hidden_states.shape[-1]
        cfg.dataset.dim_action = dim_action
        cfg.dataset.pred_horizon = pred_horizon
        cfg.dataset.exec_horizon = env_record['replan_steps']

        # Compute hand-crafted metrics
        hand_crafted_metrics = None
        if cfg.train.log_precomputed or cfg.train.log_precomputed_only:
            hand_crafted_metrics = compute_hand_crafted_metrics(
                cfg,
                policy_records=policy_records,
                exec_horizon=env_record['replan_steps'],
            )
        
        rollout = Rollout(
            hidden_states=hidden_states,
            task_suite_name=env_record["task_suite_name"],
            task_id=env_record["task_id"],
            task_description=env_record["task_description"],
            episode_idx=episode_idx,
            episode_success=int(env_record["episode_success"]),
            mp4_path=mp4_path,
            logs=hand_crafted_metrics,
            exec_horizon=env_record['replan_steps'],
            action_vectors=action_vectors,
        )
        all_rollouts.append(rollout)
    
    return all_rollouts


def compute_task_meta(
    all_rollouts: list[Rollout]
) -> dict:
    task_ids = set([r.task_id for r in all_rollouts])
    task_ids = sorted(list(task_ids))
    task_metas = {}

    for task_id in task_ids:
        task_rollouts = [r for r in all_rollouts if r.task_id == task_id]
        task_desc = task_rollouts[0].task_description
        n_rollouts = len(task_rollouts)
        n_success = sum([r.episode_success for r in task_rollouts])
        success_rate = n_success / n_rollouts if n_rollouts > 0 else 0.0
        
        task_metas[task_id] = {
            "n_rollouts": n_rollouts,
            "success_rate": success_rate,
            "task_desc": task_desc,
            "task_id": task_id,
        }

    return task_metas


def print_task_rollout_stats(
    all_rollouts: list[Rollout],
    task_metas: dict = None,
    print_rollout_lengths: bool = False,
):
    if task_metas is None:
        task_metas = compute_task_meta(all_rollouts)

    for task_id, task_meta in task_metas.items():
        task_desc = task_meta["task_desc"]
        success_rate = task_meta["success_rate"]
        n_rollouts = task_meta["n_rollouts"]
        print(f"Task ID {task_id}: {n_rollouts} rollouts, SR: {success_rate:.2f}, task_desc: {task_desc}")
        
        if print_rollout_lengths:
            # Plot the distribution of the lengths of the rollouts
            task_rollouts = [r for r in all_rollouts if r.task_id == task_id]
            rollout_lengths = [len(r.hidden_states) for r in task_rollouts]
            unique, counts = np.unique(rollout_lengths, return_counts=True)
            sort_index = np.argsort(unique)[::-1]
            unique = unique[sort_index]
            counts = counts[sort_index]
            print("Top 5 rollout lengths:", end=" ")
            for length, count in zip(unique[:5], counts[:5]):
                print(f"Len {length} count {count}; ", end="")
            print("\n")
        

def keep_topk_tasks_by_sr(
    all_rollouts: list[Rollout],
    n_tasks: int,
    task_metas: dict = None,
) -> list[Rollout]:
    if task_metas is None:
        task_metas = compute_task_meta(all_rollouts)
        
    task_id_success_rates = [(task_id, task_meta["success_rate"]) for task_id, task_meta in task_metas.items()]
    
    task_id_success_rates = sorted(task_id_success_rates, key=lambda x: x[1], reverse=True)
    keep_task_ids = [task_id for task_id, _ in task_id_success_rates[:n_tasks]]
    keep_task_ids = sorted(keep_task_ids)
    all_rollouts = [r for r in all_rollouts if r.task_id in keep_task_ids]
    # Update the task_id to be continuous
    task_id_to_new_id = {task_id: i for i, task_id in enumerate(keep_task_ids)}
    for r in all_rollouts:
        r.task_id = task_id_to_new_id[r.task_id]
    
    print("=="*20)
    print(f"After filtering, {n_tasks} tasks and {len(all_rollouts)} rollouts remain")
    
    task_ids = sorted(list(task_id_to_new_id.values()))
    for task_id in task_ids:
        task_rollouts = [r for r in all_rollouts if r.task_id == task_id]
        n_rollouts = len(task_rollouts)
        desc = task_rollouts[0].task_description
        success_rate = sum([r.episode_success for r in task_rollouts]) / n_rollouts
        print(f"Task ID {task_id}: {n_rollouts} rollouts, SR: {success_rate:.2f}, task_desc: {desc}")
        
    return all_rollouts


def ensure_task_sr_within(
    all_rollouts: list[Rollout],
    sr_lower_bound: Optional[float] = None,
    sr_upper_bound: Optional[float] = None,
    max_rollouts_per_task: Optional[int] = None,
) -> list[Rollout]:
    '''
    Ensure that the success rate (sr) of each task is within the given bounds.
    If not, remove rollouts of that task to achieve the desired success rate.

    if number of rollouts for a task is more than max_rollouts_per_task,
    randomly remove rollouts to keep the number of rollouts within the limit.
    During this removing process, remove the balanced number of successes and failures, 
    to keep the success rate to be the same. 

    Parameters:
    - all_rollouts: List of Rollout objects, each with task_id and episode_success attributes.
    - sr_lower_bound: Optional[float], minimum desired success rate (inclusive).
    - sr_upper_bound: Optional[float], maximum desired success rate (inclusive).
    - max_rollouts_per_task: Optional[int], maximum number of rollouts to keep per task.

    Returns:
    - A filtered list of Rollout. 
    '''
    if sr_lower_bound is not None and sr_upper_bound is not None and sr_lower_bound > sr_upper_bound:
        raise ValueError("sr_lower_bound must be less than or equal to sr_upper_bound")
    
    # Group rollouts by task_id
    task_to_indices = {}
    for idx, rollout in enumerate(all_rollouts):
        task_to_indices.setdefault(rollout.task_id, []).append(idx)

    keep_indices = set(range(len(all_rollouts)))

    for task_id, indices in task_to_indices.items():
        # Separate successes and failures
        successes = [i for i in indices if all_rollouts[i].episode_success == 1]
        failures = [i for i in indices if all_rollouts[i].episode_success == 0]
        total = len(successes) + len(failures)
        if total == 0:
            continue
        current_sr = len(successes) / total

        # Enforce lower bound by dropping failures
        if sr_lower_bound is not None and current_sr < sr_lower_bound and failures:
            # x failures to remove: ceil(total - successes / sr_lower_bound)
            x = int(np.ceil(total - len(successes) / sr_lower_bound))
            x = min(x, len(failures))
            to_remove = list(np.random.choice(failures, size=x, replace=False))
            keep_indices.difference_update(to_remove)
            failures = [i for i in failures if i not in to_remove]
            total -= x
            current_sr = len(successes) / total if total > 0 else 0

        # Enforce upper bound by dropping successes
        if sr_upper_bound is not None and current_sr > sr_upper_bound and successes:
            # y successes to remove: ceil((successes - sr_upper_bound * total) / (1 - sr_upper_bound))
            y = int(np.ceil((len(successes) - sr_upper_bound * total) / (1 - sr_upper_bound)))
            y = min(y, len(successes))
            to_remove = list(np.random.choice(successes, size=y, replace=False))
            keep_indices.difference_update(to_remove)
            successes = [i for i in successes if i not in to_remove]
            total -= y
            current_sr = len(successes) / total if total > 0 else 0

        # Enforce max_rollouts_per_task by balanced removal
        if max_rollouts_per_task is not None and total > max_rollouts_per_task:
            extra = total - max_rollouts_per_task
            s_count = len(successes)
            f_count = len(failures)
            # Compute how many successes/failures to drop (proportional to counts)
            if total > 0:
                s_remove = int(np.round(extra * s_count / total))
            else:
                s_remove = 0
            s_remove = min(s_remove, s_count)
            f_remove = extra - s_remove
            f_remove = min(f_remove, f_count)
            # Adjust if rounding short
            removed = s_remove + f_remove
            if removed < extra:
                remaining = extra - removed
                # Try removing from failures first
                add_f = min(remaining, f_count - f_remove)
                f_remove += add_f
                remaining -= add_f
                if remaining > 0:
                    add_s = min(remaining, s_count - s_remove)
                    s_remove += add_s
                    remaining -= add_s
            # Sample indices to remove
            to_remove_s = list(np.random.choice(successes, size=s_remove, replace=False)) if s_remove > 0 else []
            to_remove_f = list(np.random.choice(failures, size=f_remove, replace=False)) if f_remove > 0 else []
            to_remove = to_remove_s + to_remove_f
            keep_indices.difference_update(to_remove)
            successes = [i for i in successes if i not in to_remove_s]
            failures = [i for i in failures if i not in to_remove_f]
            total -= (s_remove + f_remove)
            current_sr = len(successes) / total if total > 0 else 0

    # Return filtered rollouts in original order
    return [all_rollouts[i] for i in sorted(keep_indices)]


def reorder_rollout_episode_id(rollouts: list[Rollout]) -> list[Rollout]:
    '''
    Re-order the episode ID of rollouts from each task
    '''
    task_ids = set([r.task_id for r in rollouts])
    task_ids = sorted(list(task_ids))
    
    for task_id in task_ids:
        task_rollouts = [r for r in rollouts if r.task_id == task_id]
        task_rollouts = sorted(task_rollouts, key=lambda x: x.mp4_path)
        for i, r in enumerate(task_rollouts):
            r.episode_idx = i

    return rollouts
    

def load_rollouts(cfg: Config) -> list[Rollout]:
    all_rollouts = []
    # for path in cfg.dataset.data_path:
    for path in tqdm(cfg.dataset.data_path, desc="Loading rollouts"):
        rollouts = load_rollouts_from_root(Path(path), cfg)
        all_rollouts.extend(rollouts)
        
    # Redo the task_id based on task_description for all rollouts
    task_descs = list(set([r.task_description for r in all_rollouts]))
    task_descs = sorted(task_descs)
    task_desc_to_id = {task_desc: i for i, task_desc in enumerate(task_descs)}
    for r in all_rollouts:
        r.task_id = task_desc_to_id[r.task_description]
    print(f"Found {len(task_descs)} unique tasks with total {len(all_rollouts)} rollouts")
    print_task_rollout_stats(all_rollouts, print_rollout_lengths=True)
    print("="*20)
        
        
    if cfg.dataset.full_length_only: 
        # Only keep the rollouts of the max length in each task
        indices_to_keep = []
        task_ids = sorted(list(task_desc_to_id.values()))
        for task_id in task_ids:
            task_rollouts = [r for r in all_rollouts if r.task_id == task_id]
            task_desc = task_rollouts[0].task_description
            if task_desc in TASK_DESC_TO_STANDARD_LENGTH:
                desired_length = TASK_DESC_TO_STANDARD_LENGTH[task_desc]
            else:
                desired_length = max([len(r.hidden_states) for r in all_rollouts if r.task_id == task_id])
            indices_to_keep.extend(
                [i for i, r in enumerate(all_rollouts) 
                 if r.task_id == task_id and len(r.hidden_states) == desired_length]
            )
        indices_to_keep = sorted(indices_to_keep)
        all_rollouts = [all_rollouts[i] for i in indices_to_keep]
        print(f"Only keeping the rollouts of the max length in each task, {len(all_rollouts)} rollouts remain")


    # Set the minimum step for each task, for evaluation
    all_rollouts = set_task_min_step(all_rollouts)
    
    # Compute the meta information for each task
    task_metas = compute_task_meta(all_rollouts)
    
    # Print the statistics of the task descriptions and success rates
    print_task_rollout_stats(all_rollouts, task_metas)
        
    # Exclude the tasks in EXCLUDE_TASK_DESCS
    all_rollouts = [r for r in all_rollouts if r.task_description not in EXCLUDE_TASK_DESCS]
        
    # Only keep the top n tasks sorted by success rate
    if 0 < cfg.dataset.n_tasks_used < len(task_metas):
        all_rollouts = keep_topk_tasks_by_sr(
            all_rollouts,
            n_tasks=cfg.dataset.n_tasks_used,
            task_metas=task_metas,
        )
        # Update the meta information for all tasks
        task_metas = compute_task_meta(all_rollouts)

    # Remove some rollouts to ensure the success rate of each task is within the given bounds
    all_rollouts = ensure_task_sr_within(
        all_rollouts, 
        cfg.dataset.adjust_sr_min, 
        cfg.dataset.adjust_sr_max,
        cfg.dataset.max_rollouts_per_task,
    )
    task_metas = compute_task_meta(all_rollouts)
    
    # Print the final statistics of the task descriptions and success rates
    print("="*20)
    print("Final statistics of the task descriptions and success rates:")
    print_task_rollout_stats(all_rollouts, task_metas)
    
    all_rollouts = reorder_rollout_episode_id(all_rollouts)
    

    if cfg.dataset.data_path_unseen is not None:
        # TODO: Hanlde cfg.dataset.data_path_unseen
        raise NotImplementedError("Handling cfg.dataset.data_path_unseen is not implemented yet.")

    return all_rollouts


def split_rollouts(cfg: Config, all_rollouts: list[Rollout]) -> tuple[list[Rollout], list[Rollout]]:
    if cfg.dataset.data_path_unseen is None:
        rollouts_by_split_name = split_rollouts_openvla(cfg, all_rollouts)
    else:
        # TODO: Hanlde cfg.dataset.data_path_unseen
        raise NotImplementedError("Handling cfg.dataset.data_path_unseen is not implemented yet.")
        
    return rollouts_by_split_name