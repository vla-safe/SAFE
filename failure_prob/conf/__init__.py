from dataclasses import dataclass
from typing import Any, Optional
from omegaconf import MISSING, OmegaConf
import hydra
from hydra.core.config_store import ConfigStore
import omegaconf



#########################
# Dataset Configurations
#########################

@dataclass
class DatasetConfig:
    # Common parameters for all datasets
    name: str = MISSING
    subset_name: str = "default"
    
    # If data_path_prefix is set, the data_path will be prepended with it.
    data_path_prefix: Optional[str] = None
    data_path: Any = MISSING
    # If set, unseen tasks will be loaded from this path.
    data_path_unseen: Any = None
    
    load_to_cuda: bool = True
    
    normalize_hidden_states: bool = False

    # determined by each specific dataset subclass
    pred_horizon: int = MISSING
    exec_horizon: int = MISSING
    unseen_task_ratio: float = MISSING
    seen_train_ratio: float = MISSING
    
    # for the precomputed hand-crafted metrics
    rbf_beta: float = 1.0          # bandwidth of the RBF kernel for MMD computation
    precomputed_normalize: bool = False   # If True, normalize the precomputed metrics by the length of the rollout

    # determined when initializing the dataset
    dim_features: int = MISSING # The dimension of the feature vector
    dim_action: int = MISSING # The dimension of action vector at a single timestep
    
    failure_time_label_path: Optional[str] = None # Path to annotations on failure timesteps


@dataclass
class OpenvlaDatasetConfig(DatasetConfig):
    name: str = "openvla"
    unseen_task_ratio: float = 0.3
    seen_train_ratio: float = 0.6
    pred_horizon: int = 1
    exec_horizon: int = 1

    # The index of the token feature to use for failure probing
    token_idx_rel: float | str = 1.0
    
    

@dataclass
class OpenPizeroDatasetConfig(DatasetConfig):
    name: str = "open_pizero"
    unseen_task_ratio: float = 0.25
    seen_train_ratio: float = 0.66
    pred_horizon: int = 4
    exec_horizon: int = 2

    feat_name: str = "sampled_action_embeds"
    diff_idx_rel: float | str = 0.0
    horizon_idx_rel: float | str = 0.0


@dataclass
class PizeroFastDatasetConfig(DatasetConfig):
    '''
    This is also for the LIBERO benchmark, but the data is generated by pi0-fast policy.
    '''
    name: str = "pizero_fast"
    
    unseen_task_ratio: float = 0.3  
    seen_train_ratio: float = 0.6
    pred_horizon: int = MISSING
    exec_horizon: int = MISSING  # This will be decided from rollout meta information

    # which feature to use for failure probing
    feat_name: str = "pre_logits"
    # Index into the predicted token, as a ratio of the total number of tokens (0 means the first token, 1 means the last token)
    token_idx_rel: float | str = "mean"
    
    
@dataclass
class PizeroFastDroidDatasetConfig(DatasetConfig):
    '''
    This is for the real-robot rollouts collected by pi0-fast policy.
    '''
    name: str = "pizero_fast_droid"
    
    data_path: list[str] = MISSING
    data_path_unseen: Optional[list[str]] = None
    
    n_tasks_used: int = 0 # 0 means all tasks
    full_length_only: bool = True # If True, only keep the rollouts with max length

    # If set, drop rollouts to lower or upper bound SR for each task
    adjust_sr_min: Optional[float] = None
    adjust_sr_max: Optional[float] = None
    max_rollouts_per_task: Optional[int] = None
    
    unseen_task_ratio: float = 0.3
    seen_train_ratio: float = 0.6
    pred_horizon: int = MISSING
    exec_horizon: int = MISSING  # This will be decided from rollout meta information

    # which feature to use for failure probing
    feat_name: str = "pre_logits"
    # Index into the predicted token, as a ratio of the total number of tokens (0 means the first token, 1 means the last token)
    token_idx_rel: float | str = "mean"
    
    
@dataclass
class PizeroDatasetConfig(DatasetConfig):
    '''
    This is also for the LIBERO benchmark, but the data is generated by pi0-fast policy.
    '''
    name: str = "pizero"
    
    unseen_task_ratio: float = 0.3  
    seen_train_ratio: float = 0.6
    pred_horizon: int = MISSING
    exec_horizon: int = MISSING  # This will be decided from rollout meta information

    # which feature to use for failure probing
    feat_name: str = "pre_velocity"
    diff_idx_rel: float | str = "mean"
    horizon_idx_rel: float | str = "mean"
    

#########################
# Model Configurations
#########################

@dataclass
class ModelConfig:
    # Common parameters for all models
    name: str = MISSING
    use_time_weighting: bool = False

    n_epochs: int = 1000
    batch_size: int = 512

    optimizer: str = "adam"
    lr: float = 1e-3
    lr_step_size: int = 300
    lr_gamma: float = 1.0 # by default, no decay
    weight_decay: float = 1e-2 # Only used for AdamW optimizer
    warmup_steps: int = 0 # If set, use a linear warmup schedule for the learning rate
    
    lambda_success: float = 1.0
    lambda_fail: float = 1.0
    lambda_reg: float = 1.0
    cumsum: bool = MISSING
    rmean: bool = MISSING
    
    init_weight_scale: float = 1.0
    grad_max_norm: Optional[float] = None
    dropout: float = 0.0

@dataclass
class IndepModelConfig(ModelConfig):
    name: str = "indep"
    n_layers: int = 2
    hidden_dim: int = 256
    final_act_layer: str = "sigmoid"
    n_history_steps: int = 1 # 1 means only consider the current timestep
    
    use_threshold: bool = False
    threshold: float = 50

    cumsum: bool = True
    rmean: bool = False

@dataclass
class LstmModelConfig(ModelConfig):
    name: str = "lstm"
    n_layers: int = 1
    hidden_dim: int = 256
    n_history_steps: int = -1 # -1 means use all history information
    one_loss_per_seq: bool = False

    lr: float = 3e-4
    lambda_reg: float = 1.0
    lambda_hard_heg: float = 0.0
    hard_neg_margin: float = 0.1
    hard_neg_beta: float = 50.0
    
    cumsum: bool = False
    rmean: bool = False
    
@dataclass
class EmbedModelConfig(ModelConfig):
    name: str = "embed"
    n_epochs: int = 1
    distance: str = "mahala"
    
    # When distance in ["cosine", "euclidean"]
    topk: int = 10
    
    # When distance is "pca-kmeans"
    pca_dim: int = 64
    n_clusters: int = 16
    
    use_success_only: bool = False
    cumsum: bool = True
    
@dataclass
class RNDModelConfig(ModelConfig):
    name: str = "rnd"

    n_epochs: int = 200
    batch_size: int = 64
    lr: float = 1e-4
    lambda_reg: float = 0.0

    use_success_only: bool = False
    

@dataclass
class LogpZOModelConfig(ModelConfig):
    name: str = "logpZO"

    n_epochs: int = 200
    batch_size: int = 24
    lr: float = 1e-4
    lambda_reg: float = 0.0
    
    forward_chunk_size: int = 0

    use_success_only: bool = False
    
    in_dim: int = 20
    

#########################
# Training Configurations
#########################

@dataclass
class TrainConfig:
    log_precomputed: bool = False
    log_precomputed_only: bool = False
    seed: str | int = "0"
    wandb_project: str = "vla-safe"
    wandb_dir: str = "./wandb"
    wandb_group_name: str = "debug"
    exp_suffix: str = "debug"
    exp_name: str = MISSING
    debug: bool = False
    
    vis_every: int = 500
    roc_every: int = 25
    
    eval_save_video: bool = False
    eval_save_video_functional: bool = False
    eval_save_video_multiproc: bool = True
    eval_save_timing_plots: bool = False
    eval_save_logs: bool = False
    eval_save_ckpt: bool = False
    logs_save_root: str = "./logs/"
    logs_save_path: str = MISSING
    eval_cp_alpha: float = 0.2
    
    

#########################
# Top-Level Configuration
#########################

@dataclass
class Config:
    # Instead of a flat config, we now compose our configuration from three parts.
    # These fields are required and will be provided by Hydraâ€™s config groups.
    dataset: DatasetConfig = MISSING
    model: ModelConfig = MISSING
    train: TrainConfig = TrainConfig()

#########################
# Register with ConfigStore
#########################

cs = ConfigStore.instance()
cs.store(name="base_config", node=Config)

# Register dataset variants into the "dataset" config group.
cs.store(group="dataset", name="base_openvla", node=OpenvlaDatasetConfig)
cs.store(group="dataset", name="base_open_pizero", node=OpenPizeroDatasetConfig)
cs.store(group="dataset", name="base_pizero_fast", node=PizeroFastDatasetConfig)
cs.store(group="dataset", name="base_pizero", node=PizeroDatasetConfig)
cs.store(group="dataset", name="base_pizero_fast_droid", node=PizeroFastDroidDatasetConfig)

# Register model variants into the "model" config group.
cs.store(group="model", name="base_indep", node=IndepModelConfig)
cs.store(group="model", name="base_lstm", node=LstmModelConfig)
cs.store(group="model", name="base_embed", node=EmbedModelConfig)
cs.store(group="model", name="base_rnd", node=RNDModelConfig)
cs.store(group="model", name="base_logpzo", node=LogpZOModelConfig)

# Register the train config
cs.store(group="train", name="base_train", node=TrainConfig)


def process_cfg(cfg: Config):
    if cfg.dataset.data_path_prefix is not None:
        # Prepend the data_path with the data_path_prefix
        if isinstance(cfg.dataset.data_path, omegaconf.listconfig.ListConfig):
            for i in range(len(cfg.dataset.data_path)):
                cfg.dataset.data_path[i] = cfg.dataset.data_path_prefix + cfg.dataset.data_path[i]
        elif isinstance(cfg.dataset.data_path, str):
            cfg.dataset.data_path = cfg.dataset.data_path_prefix + cfg.dataset.data_path
        else:
            raise ValueError(f"DatasetConfig: data_path should be a string or a list of strings, but got {type(cfg.dataset.data_path)}")
        print(f"DatasetConfig: set data_path to {cfg.dataset.data_path}")

        # Prepend the data_path_unseen with the data_path_prefix if it is set
        if cfg.dataset.data_path_unseen is not None:
            if isinstance(cfg.dataset.data_path_unseen, omegaconf.listconfig.ListConfig):
                for i in range(len(cfg.dataset.data_path_unseen)):
                    cfg.dataset.data_path_unseen[i] = cfg.dataset.data_path_prefix + cfg.dataset.data_path_unseen[i]
            elif isinstance(cfg.dataset.data_path_unseen, str):
                cfg.dataset.data_path_unseen = cfg.dataset.data_path_prefix + cfg.dataset.data_path_unseen
            else:
                raise ValueError(f"DatasetConfig: data_path_unseen should be a string or a list of strings, but got {type(cfg.dataset.data_path_unseen)}")
            print(f"DatasetConfig: set data_path_unseen to {cfg.dataset.data_path_unseen}")

    return cfg